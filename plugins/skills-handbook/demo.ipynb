{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skills Handbook Plugin Demo\n",
    "\n",
    "This notebook demonstrates how to use the Skills Handbook plugin with the Portkey Python SDK.\n",
    "\n",
    "The Skills Handbook plugin enhances AI responses by retrieving relevant positive and negative examples from a Qdrant vector database and injecting them as context before the LLM processes the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. Portkey Gateway running at `http://localhost:8787`\n",
    "2. Qdrant instance running at `http://localhost:6333` with collections populated\n",
    "3. OpenAI API key for both LLM calls and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install portkey-ai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'portkey_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mportkey_ai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Portkey\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load environment variables\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'portkey_ai'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from portkey_ai import Portkey\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GATEWAY_URL = \"http://localhost:8787\"\n",
    "QDRANT_URL = \"http://localhost:6333\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Skills Handbook Plugin Demo\n\nThis notebook demonstrates how to use the Skills Handbook plugin with the Portkey Python SDK.\n\nThe Skills Handbook plugin enhances AI responses by retrieving relevant positive and negative examples from a Qdrant vector database and injecting them as context before the LLM processes the request.\n\n## Key Feature: Dynamic Config Per Request\nYou can pass different plugin configurations on each request, giving you full control over when and how to use the memory/handbook features!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the plugin\n",
    "config = {\n",
    "    \"strategy\": {\"mode\": \"single\"},\n",
    "    \"targets\": [\n",
    "        {\n",
    "            \"provider\": \"openai\",\n",
    "            \"api_key\": OPENAI_API_KEY\n",
    "        }\n",
    "    ],\n",
    "    \"input_guardrails\": [\n",
    "        {\n",
    "            \"id\": \"skills-handbook-retrieval\",\n",
    "            \"type\": \"mutator\",\n",
    "            \"skills-handbook.handbook\": {\n",
    "                \"credentials\": {\n",
    "                    \"endpoint\": QDRANT_URL,\n",
    "                    \"apiKey\": \"\",  # Empty for local Qdrant\n",
    "                    \"openaiApiKey\": OPENAI_API_KEY\n",
    "                },\n",
    "                \"positiveCollectionName\": \"skills-handbook-positive\",\n",
    "                \"negativeCollectionName\": \"skills-handbook-negative\",\n",
    "                \"topK\": 3,\n",
    "                \"scoreThreshold\": 0.5,\n",
    "                \"includePositive\": True,\n",
    "                \"includeNegative\": True\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Initialize Portkey client\n",
    "portkey = Portkey(\n",
    "    base_url=GATEWAY_URL,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Make a request about API authentication\n",
    "response = portkey.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How should I handle API authentication in my application?\"\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"Response with Skills Handbook:\")\n",
    "print(\"=\" * 80)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Example 2: Dynamic Config Per Request (Recommended!)\n\nThe most flexible way to use the plugin is to pass the config directly in each request. This lets you dynamically control when to use memory without creating multiple clients.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Initialize a single Portkey client - no config needed upfront!\nportkey = Portkey(\n    base_url=GATEWAY_URL,\n    api_key=OPENAI_API_KEY\n)\n\nprint(\"Example 2A: Request WITHOUT memory\")\nprint(\"=\" * 80)\n\n# Request 1: No config = no memory/handbook\nresponse1 = portkey.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"How do I handle API authentication?\"}],\n    model=\"gpt-3.5-turbo\",\n    max_tokens=150\n)\n\nprint(response1.choices[0].message.content)\nprint()\n\nprint(\"=\" * 80)\nprint(\"Example 2B: Request WITH memory - pass config directly\")\nprint(\"=\" * 80)\n\n# Request 2: Pass config = use memory/handbook\nresponse2 = portkey.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"How do I handle API authentication?\"}],\n    model=\"gpt-3.5-turbo\",\n    max_tokens=150,\n    config={\n        \"input_guardrails\": [{\n            \"id\": \"skills-handbook\",\n            \"type\": \"mutator\",\n            \"skills-handbook.handbook\": {\n                \"credentials\": {\n                    \"endpoint\": QDRANT_URL,\n                    \"apiKey\": \"\",\n                    \"openaiApiKey\": OPENAI_API_KEY\n                },\n                \"positiveCollectionName\": \"skills-handbook-positive\",\n                \"negativeCollectionName\": \"skills-handbook-negative\",\n                \"topK\": 3,\n                \"scoreThreshold\": 0.5,\n                \"includePositive\": True,\n                \"includeNegative\": True\n            }\n        }]\n    }\n)\n\nprint(response2.choices[0].message.content)\nprint()\n\nprint(\"=\" * 80)\nprint(\"Example 2C: Another request with DIFFERENT memory settings\")\nprint(\"=\" * 80)\n\n# Request 3: Different config = different behavior\nresponse3 = portkey.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"What API mistakes should I avoid?\"}],\n    model=\"gpt-3.5-turbo\",\n    max_tokens=150,\n    config={\n        \"input_guardrails\": [{\n            \"id\": \"skills-handbook\",\n            \"type\": \"mutator\",\n            \"skills-handbook.handbook\": {\n                \"credentials\": {\n                    \"endpoint\": QDRANT_URL,\n                    \"apiKey\": \"\",\n                    \"openaiApiKey\": OPENAI_API_KEY\n                },\n                \"topK\": 5,  # More examples\n                \"scoreThreshold\": 0.6,  # Higher threshold\n                \"includePositive\": False,  # Only show negative examples\n                \"includeNegative\": True\n            }\n        }]\n    }\n)\n\nprint(response3.choices[0].message.content)\nprint(\"=\" * 80)\n\nprint(\"\\n✅ All three requests used the SAME client, but with different configs!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Comparison - With vs Without Plugin\n",
    "\n",
    "Let's compare responses with and without the Skills Handbook plugin to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request WITHOUT the plugin\n",
    "portkey_no_plugin = Portkey(\n",
    "    base_url=GATEWAY_URL,\n",
    "    config={\n",
    "        \"strategy\": {\"mode\": \"single\"},\n",
    "        \"targets\": [{\"provider\": \"openai\", \"api_key\": OPENAI_API_KEY}]\n",
    "    }\n",
    ")\n",
    "\n",
    "question = \"What should I avoid when making API calls?\"\n",
    "\n",
    "# Response without plugin\n",
    "response_no_plugin = portkey_no_plugin.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "# Response with plugin\n",
    "response_with_plugin = portkey.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WITHOUT Skills Handbook Plugin:\")\n",
    "print(\"=\"*80)\n",
    "print(response_no_plugin.choices[0].message.content)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WITH Skills Handbook Plugin:\")\n",
    "print(\"=\"*80)\n",
    "print(response_with_plugin.choices[0].message.content)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Different Topics\n",
    "\n",
    "The plugin retrieves context based on semantic similarity. Let's test with different API-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"How do I handle API rate limits?\",\n",
    "    \"What's the best way to store API credentials?\",\n",
    "    \"Should I use pagination when fetching data from APIs?\",\n",
    "    \"How should I handle API errors?\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions, 1):\n",
    "    response = portkey.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Customizing Plugin Behavior\n",
    "\n",
    "You can customize various aspects of the plugin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with custom parameters\n",
    "custom_config = {\n",
    "    \"strategy\": {\"mode\": \"single\"},\n",
    "    \"targets\": [{\"provider\": \"openai\", \"api_key\": OPENAI_API_KEY}],\n",
    "    \"input_guardrails\": [\n",
    "        {\n",
    "            \"id\": \"skills-handbook-custom\",\n",
    "            \"type\": \"mutator\",\n",
    "            \"skills-handbook.handbook\": {\n",
    "                \"credentials\": {\n",
    "                    \"endpoint\": QDRANT_URL,\n",
    "                    \"apiKey\": \"\",\n",
    "                    \"openaiApiKey\": OPENAI_API_KEY\n",
    "                },\n",
    "                \"positiveCollectionName\": \"skills-handbook-positive\",\n",
    "                \"negativeCollectionName\": \"skills-handbook-negative\",\n",
    "                \"topK\": 5,  # Retrieve more examples\n",
    "                \"scoreThreshold\": 0.6,  # Higher threshold for more relevant results\n",
    "                \"includePositive\": True,\n",
    "                \"includeNegative\": False,  # Only positive examples\n",
    "                \"positivePrefix\": \"\\n## Best Practices:\\n\",\n",
    "                \"positiveSuffix\": \"\\n---\\n\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "portkey_custom = Portkey(\n",
    "    base_url=GATEWAY_URL,\n",
    "    config=custom_config\n",
    ")\n",
    "\n",
    "response = portkey_custom.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What are API best practices?\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=250\n",
    ")\n",
    "\n",
    "print(\"Response with customized plugin settings:\")\n",
    "print(\"=\"*80)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Only Negative Examples\n",
    "\n",
    "Sometimes you might want to focus only on what NOT to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration with only negative examples\n",
    "negative_only_config = {\n",
    "    \"strategy\": {\"mode\": \"single\"},\n",
    "    \"targets\": [{\"provider\": \"openai\", \"api_key\": OPENAI_API_KEY}],\n",
    "    \"input_guardrails\": [\n",
    "        {\n",
    "            \"id\": \"skills-handbook-negative-only\",\n",
    "            \"type\": \"mutator\",\n",
    "            \"skills-handbook.handbook\": {\n",
    "                \"credentials\": {\n",
    "                    \"endpoint\": QDRANT_URL,\n",
    "                    \"apiKey\": \"\",\n",
    "                    \"openaiApiKey\": OPENAI_API_KEY\n",
    "                },\n",
    "                \"positiveCollectionName\": \"skills-handbook-positive\",\n",
    "                \"negativeCollectionName\": \"skills-handbook-negative\",\n",
    "                \"topK\": 3,\n",
    "                \"scoreThreshold\": 0.5,\n",
    "                \"includePositive\": False,  # Disable positive examples\n",
    "                \"includeNegative\": True    # Only negative examples\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "portkey_negative = Portkey(\n",
    "    base_url=GATEWAY_URL,\n",
    "    config=negative_only_config\n",
    ")\n",
    "\n",
    "response = portkey_negative.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What mistakes should I avoid when working with APIs?\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"Response with only negative examples:\")\n",
    "print(\"=\"*80)\n",
    "print(response.choices[0].message.content)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Streaming Responses\n",
    "\n",
    "The plugin also works with streaming responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Streaming response with Skills Handbook:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stream = portkey.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Give me tips for API security\"}],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    max_tokens=200,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Skills Handbook plugin:\n",
    "\n",
    "✅ **Enhances AI responses** with relevant examples from your knowledge base\n",
    "\n",
    "✅ **Uses semantic search** via OpenAI embeddings and Qdrant\n",
    "\n",
    "✅ **Supports both positive and negative examples** to guide behavior\n",
    "\n",
    "✅ **Fully customizable** - adjust retrieval count, thresholds, formatting, etc.\n",
    "\n",
    "✅ **Works with streaming** and all OpenAI-compatible models\n",
    "\n",
    "### Use Cases:\n",
    "- **Team Knowledge Sharing**: Store best practices that guide AI responses\n",
    "- **Guardrails**: Prevent common mistakes by including negative examples\n",
    "- **Domain Expertise**: Inject specialized knowledge for specific tools/domains\n",
    "- **Consistent Behavior**: Ensure AI agents follow organizational standards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}